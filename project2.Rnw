\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,float,amsfonts,enumitem,tcolorbox,hyperref}
\usepackage[left=0.5in,right=0.5in,top=1in,bottom=1in,footskip=.25in]{geometry}
\usepackage{float}
\include{macros}

<<{r  global_options},echo=FALSE>>=
knitr::opts_chunk$set(include=TRUE,echo = F, message = FALSE, warning = FALSE,
                      fig.width=3, fig.height=3, fig.align='center',
                      fig.asp = 1,dev = 'png', dpi = 100,fig.retina = 2, cache = T
                      )
@

\title{STAT 154: Project 2 Cloud Data}
\author{\textbf{Shrey Samdani}: 3032000414}
\date{\textbf{Man Chong Chan(Aaron)}: 3033281903 }

\begin{document}

\maketitle

% \section*{Please read carefully!}
% \begin{itemize}
%   \item It is a good idea to revisit your notes, slides and reading;
% and synthesize their main points BEFORE doing the project.
%   \item \emph{For this project, we adapt a zero tolerance policy with 
%   incorrect/late submissions (no emails please) to Gradescope.}
%   \item The recommended work of this project is at least 20 hours (at least 10 hours / person). Plan ahead and start early. 
%   \item We need two things:
%   \begin{enumerate}[label=(\alph*)]
%     \item A main pdf report \textbf{(font size at least 11 pt, less or equal
%     to 12 pages)} generated by Latex, Rnw or Word is required to be
%     submitted to Gradescope.
%     \begin{itemize}
%       \item Provide top class (research-paper level) writing, useful
%     well-labeled figures and no code in this pdf. Arrange text and figures
%     compactly (.Rnw may not be very useful for this).
%     \item You can choose a title for the report and a team name as per your
%     liking (\emph{get creative!}). Do provide the names and student ID of
%     your teammates below the title.
%     \item Your report should conclude with an acknowledgment section, where
%     you provide brief discussion about the contributions of each member,
%     \textbf{and} the resources you used, credit all the help you took
%     and briefly outline the way you proceeded with the project.
%     \end{itemize}
%     \item A link to your GitHub Repo at the end of your write-up that contains
%     all your code (see Section 5 for more details).
%   \end{enumerate}
%   \item \textbf{Be visual \emph{and} quantitative:} Remember projects are graded differently when compared to homework---one line answer without explanation is usually not enough. Make your findings succinct and try to convince us with good arguments supported by numbers and figures.
% Putting yourself in reader's shoes and reading the report out loud usually helps. The standards for grading are \emph{very high} this time. We will be very picky with figures: Lack of proper titles and axis labels will lead to loss of several points.
%   
% \end{itemize}
% 
% \newpage
% 
% \section*{Overview of the project} % (fold)
% \label{sec:overview_of_the_project}
% 
% The goal of this project is the exploration and modeling of cloud detection in 
% the polar regions based on radiance recorded automatically by the MISR sensor 
% abroad the NASA satellite Terra. You will attempt to build a classification 
% model to distinguish the presence of cloud from the absence of clouds in
% the images using the available signals/features. Your dataset has ``expert
% labels'' that can be used to train your models. When you evaluate your
% results, imagine that your models will be used to distinguish
% clouds from non-clouds on a large number of images that won't have these 
% ``expert'' labels.
% 
% On Piazza, you will find a zip archive with three files: \textbf{image1.txt},
% \textbf{image2.txt}, \textbf{image3.txt}. Each contains one picture from
% the satellite. Each of these files contains several rows each with 11 columns 
% described in the Table~\ref{tab:feature} below. All five radiance angles
% are raw features, while NDAI, SD, and CORR are features that are computed
% based on subject matter knowledge. More information about the features is
% in the article \textbf{yu2008.pdf}. The sensor data is multi-angle and recorded
% in the red-band. 
% For more information about MISR, see \textbf{http://www-misr.jpl.nasa.gov/}.
% 
% \begin{table}[h]
% \centering
% \begin{tabular}{|c|c|}
% \hline
%  01 & y coordinate \\ \hline
%  02 & x coordinate \\ \hline
%  03 & expert label (+1 = cloud, -1 = not cloud, 0 unlabeled)\\ \hline
%  04 & NDAI \\ \hline
%  05 & SD \\ \hline
%  06 & CORR \\ \hline
%  07 & Radiance angle DF\\ \hline
%  08 & Radiance angle CF\\ \hline
%  09 & Radiance angle BF\\ \hline
%  10 & Radiance angle AF\\ \hline
%  11 & Radiance angle AN\\ \hline
% \end{tabular}
% \label{tab:feature}
% \caption{Features in the cloud data.}
% \end{table}

\section{Data Collection and Exploration (30 pts)}
\begin{enumerate}[label=(\alph*)]
\item \textbf{Write a half-page summary} of the paper, including at least
the purpose of the study, the data, the collection method, its conclusions
and potential impact.\\\\
\qquad This article proposes two new operational Arctic cloud detection algorithms: ELCM and ELCM-QDA using Multiangle Imaging SpectroRadiometer (MISR) imagery. The key idea is to identify cloud-free surface pixels in the imagery instead of cloudy pixels as in the existing MISR operational algorithms. The data used in this study were collected from 10 MISR orbits of path 26 over the Arctic, northern Greenland, and Baffin Bay. Path 26 was chosen for the study because it includes permanent sea ice in the Arctic Ocean, snow-covered and snow-free coastal mountains in Greenland, permanent glacial snow and ice, and sea ice that melted across Baffin Bay over the study period. Through extensive exploratory data analysis and using domain knowledge, three physically useful features: CORR, SD, NDAI have been identified, which are vital to the new algorithm.\\ \quad The ELCM algorithm based on the three features is more accurate and provides better spatial coverage than the existing MISR operational algorithms for cloud detection in the Arctic. Moreover, results from the ELCM algorithm can be used to train QDA to provide probability labels for partly cloudy scenes.\\\quad In this study, unlike the past, statisticians are directly involved in the data processing. The success of the study was achieved by the collaborations between atmospheric scientists, statisticians, and the MISR science and instrument teams at the Jet Propulsion Laboratory.  This demonstrates the contribution of statisticians in the analysis of the study. The second significant aspect of this research is that it demonstrates the power of statistical thinking, and also the ability of statistics to contribute solutions to modern scientific problems. 

<<>>=
colnames = c('x','y','label','NDAI','SD','CORR','DF','CF','BF','AF','AN')
data1 = read.table("image_data/image1.txt", col.names = colnames)
data1$image = 'data1'

data2 = read.table("image_data/image2.txt", col.names = colnames)
data2$image = 'data2'
data3 = read.table("image_data/image3.txt", col.names = colnames)
data3$image = 'data3'
allData = rbind(data1,data2,data3)
@

\item \textbf{Summarize} the data, i.e., $\%$ of pixels for the different
classes. \textbf{Plot well-labeled beautiful maps} using $x, y$ coordinates
the expert labels with color of the region based on the expert labels.\\
We can see a summary of the data below: 
<<>>=
summary(allData[,-12])
@
There are 11 features for each data point. They are x, y, label, NDAI, SD, CORR, DF, CF, BF, AF and AN. x and y are just the coordinate of the pixel in its corresponding image; label is the true label provided by expert; NDAI, SD and CORR are features that had been identified through extensive exploratory data analysis; DF, CF, BF, AF and AN represent different radiance angles. The radiance angles also use units that are much greater in magnitdue than the first three features.\\
\quad For image1, there are 0.44 of pixels labeled as -1 (not cloud), 0.38 as 0 (unlabeled) and 0.18 as 1 (cloud). For image2, there are 0.37 of pixels labeled as -1 (not cloud), 0.29 as 0 (unlabeled) and 0.34 as 1 (cloud). Finally for image3, there are 0.29 of pixels labeled as -1 (not cloud), 0.52 as 0 (unlabeled) and 0.18 as 1 (cloud).



\textbf{Do you observe some trend/pattern? Is an i.i.d. assumption for
the samples justified for this dataset?}
<<fig.asp=1/3,fig.width=10>>=
library(ggplot2)
ggplot(allData, aes(x=x,y=y,color = as.factor(label))) + geom_point() + facet_wrap(~image) +
  labs(title="Region Plot Based on Expert Label")+
  scale_color_manual(labels = c("no cloud", "unlabeled", "cloud"), values = c("skyblue", "black", "white")) +
  guides(color=guide_legend("expert label"))
@
Looking at the plot based on the expert labels, we can definitely see that there are clusters for each of the classification labels, meaning that data are not iid. For example, if you know the label of one data point, the labels of the nearby data points are more likely to have the same label. In other words, there exists spatial dependence between data points.

\item \textbf{Perform a visual and quantitative EDA} of the dataset, e.g.,
summarizing (i) pairwise relationship between the features themselves and
(ii) the relationship between the expert labels with the individual features.
<<>>=

@

<<fig.width=7, out.width='60%'>>=
data.labeled = allData[allData$label != 0,]
library(corrplot)
correlation = cor(data.labeled[,-12])
corrplot.mixed(correlation)
@
Many of the radiance readings(DF, CF, BF, AF, AN) have high correlation, which is reasonable as they are simply different angles of the same picture. There is a high linear correlation between NDAI/CORR and label. This suggests that a linear method of classification might be useful in predicting the labels. The y coordinate has a relatively high correlation with the label, although given its definition, this may be merely a coincidence.\\


\textbf{Do you notice differences} between the two classes (cloud, no cloud)
based on the radiance or other features (CORR, NDAI, SD)?
<<fig.width=6,fig.asp=1/3, out.width='90%'>>=
par(mfrow = c(1, 3))
hist(allData$NDAI[allData$label==1], main = "NDAI of cloud", xlab = "value")
hist(allData$SD[allData$label==1], main = "SD of cloud", xlab = "value")
hist(allData$CORR[allData$label==1], main = "CORR of cloud", xlab = "value")

par(mfrow = c(1, 3))
hist(allData$NDAI[allData$label==-1], main = "NDAI of no cloud", xlab = "value")
hist(allData$SD[allData$label==-1], main = "SD of no cloud", xlab = "value")
hist(allData$CORR[allData$label==-1], main = "CORR of no cloud", xlab = "value")
@

Based on the distribution of NDAI given different labels, we found that the NDAI of cloudy pixels tend to be higher with mean = 1.95, while NDAI of non-cloudy pixels have mean = -0.26.\\
Similarly for SD, we found that the SD of cloudy pixels also tend to be higher with mean = 9.84, while SD of non-cloudy pixels have mean = 2.98
\\For CORR, there is no obvious difference between different labels; their means are not significantly different; the two means are 0.26 and 0.14 respectively. 
\end{enumerate}


\section{Preparation (40 pts)}
Now that we have done EDA with the data, we now prepare to train our model.
\begin{enumerate}[label=(\alph*)]
\item (Data Split) \textbf{Split the entire data} (image1.txt, image2.txt,
image3.txt) into three sets: training,  validation and test. Think carefully
about how to split the data. \textbf{Suggest at least two non-trivial different
ways} of splitting the data which takes into account that the data is not i.i.d.\\\\
As we concluded in 1b: there exists spatial dependence between data points. However, we want the data to be independent. If spatial dependence exists between data points in the training set and test set, this might result in a model with a high accuracy. Such models might perform well on the data that we have because they take advantages of the spatial dependence but they might not be a good model for future data. Future data will come in as a chunk of pixels and they will be likely to have different level of spatial dependence between pixels. If our model predicts the future data based on the spatial dependence of the training data, the prediction will probably be inaccurate.
To tackle this issue, we have come up with two different methods.\\\\
Method 1:\\
	Since future data will be likely be formatted as a chunk of pixels, we decided to divide each of the pictures into 10x10 blocks. This would give us approximately 3596 blocks of data. Then, we randomly chose 80$\%$ of the blocks as our training set; 20$\%$ as our test set; 20$\%$ of the training set as our validation set. By dividing the data into blocks, we hope that this would eliminate the spatial dependence between blocks, as pixels in different blocks will not be dependent on each other.\\\\
	Method 2:\\
	Since future data might come in as another picture and we happen to have 3 pictures, we decided to assign the three pictures as our training set, validation set and test set respectively. Since the three pictures are completely different images, they are spatially independent of each other. This would solve the problem of spatial dependence between training set, validation set and test set.
<<>>=
set.seed(123)
data1 = data.labeled[data.labeled$image == 'data1',]
data2 = data.labeled[data.labeled$image == 'data2',]
data3 = data.labeled[data.labeled$image == 'data3',]


# METHOD 1 (divide by blocks)
BLOCK_SIZE = 10
blocks = list()
images = list(data1,data2,data3)
for (i in 1:3) {
  image = images[[i]]
  x = range(image$x)
  x.coordinates = seq(x[1],x[2],BLOCK_SIZE)
  y = range(image$y)
  y.coordinates = seq(y[1],y[2],BLOCK_SIZE)
  for (x_coord in x.coordinates) {
    for (y_coord in y.coordinates) {
      blocks[[length(blocks)+1]] = c(i,x_coord,y_coord)
    }
  }
}

TEST_PROP = 0.2
VALIDATION_PROP = (1-TEST_PROP)*0.2
n = length(blocks)

random_blocks = sample(blocks)
test = random_blocks[1:as.integer(n*TEST_PROP)]
random_blocks = random_blocks[-(1:as.integer(n*TEST_PROP))]
validation = random_blocks[1:as.integer(n*VALIDATION_PROP)]


library(dplyr)
test.data = data.frame(data1[0,])
for (tBlock in test) {
  image = images[[tBlock[1]]]
  x = tBlock[2]
  y = tBlock[3]
  test.data = rbind(test.data,image[between(image$x,x,x+9) & between(image$y,y,y+9),])
}

val.data = data.frame(data1[0,])
for (tBlock in validation) {
  image = images[[tBlock[1]]]
  x = tBlock[2]
  y = tBlock[3]
  val.data = rbind(val.data,image[between(image$x,x,x+9) & between(image$y,y,y+9),])
}

train.data = setdiff(data.labeled,test.data)
train.data = setdiff(train.data, val.data)
@


\item (Baseline) \textbf{Report the accuracy of a trivial classifier} which
sets all labels to -1 (cloud-free) on the validation set and on the test set. 
In what scenarios will such a classifier have high average accuracy?
\emph{Hint: Such a step provides a baseline to ensure that the classification
problems at hand is not trivial.}\\\\
Such a trivial classifier gives accuracy 71\% and 61\% for the validation set and test set respectively. This classifier would have a high average accuracy if the new data given were mostly non-cloudy area.

\item (First order importance) Assuming the expert labels as the
truth, and without using fancy classification methods, suggest
three of the ``best`` features, \textbf{using quantitative and visual justification}. Define your ``best`` feature criteria clearly. Only the relevant plots are necessary. Be sure to give this careful consideration, as it relates to subsequent problems.
<<>>=
x = data.matrix(data.labeled[,c(-1,-2,-3,-12)])
y = data.matrix(data.labeled[,3])
library(glmnet)
model = glmnet(x,y,alpha = 1, lambda=.1)
print("LASSO REGRESSION COEFFICIENTS (lambda = 0.1)")
coef(model)
print(paste("OLS REGRESSION COEFFICIENT:",sqrt(summary(lm(label~.,data=data.labeled))$r.squared)))
@
Looking at the lasso regression, we can see that the important features are NDAI and CORR. This is somewhat reliable since we can see that the regression coefficient of the OLS estimate is at 0.8, suggesting a somewhat strong linear correlation between the featues and the label. This inference is also supported by the high correlations between NDAI/label and CORR/label (shown above). We also know from the above correlations that the one of the radiance readings should be sufficient to represent all of the readings since they are all highly correlated. Looking at the correlations, we see that BF has the highest sum of correlations between the radiance readings, so we choose the features NDAI, CORR, and BF to represent our data.

\item Write a generic cross validation (CV) function \textbf{CVgeneric} in R that takes a generic classifier, training features, training labels, number of folds $K$ and a loss function (at least classification accuracy should be there) as inputs and outputs the $K$-fold CV loss on the training set.  Please remember to put it in your github folder in Section 5.
\end{enumerate}

\section{Modeling (40 pts)}
We now try to fit different classification models and assess the fitted
models using different criterion. For the next three parts, we expect you
to try \emph{logistic regression and at least three other methods}.
\begin{enumerate}[label=(\alph*)]
\item \textbf{Try several classification methods and assess their fit using
cross-validation (CV). Provide a commentary on the assumptions for the
methods you tried and if they are satisfied in this case.} 
Since CV does not have a validation set, you can merge your training and
validation set to fit your CV model. 
\textbf{Report} the accuracies across
folds (and not just the average across folds) and the test accuracy. CV-results
for both the ways of creating folds (as answered in part 2(a)) should be
reported. Provide a brief commentary on the results. Make sure you honestly
mention all the classification methods you have  tried.\\\\
We ran tests on 5 models (QDA, LDA, Logistic Regression, Decision Tree, Kernel SVM - RBF) using 4 fold CV for method 1 and 2 fold CV for method 2. For method 2, we are bounded by 2 folds since our training set is only 2 images.
<<>>=
source('crossval.R')

K = 4
data = rbind(train.data, val.data)
data$label = (data$label+1)/2
formula = as.formula(label~.)
hyperparams = list('sigma' = 1,'C'=1)

noLoss = function(x,y){0}
library(MLmetrics)

models = c('QDA' ,'LDA','logistic','dtree', 'kernelSVM')
model.accuracies = data.frame(matrix(ncol = K+2, nrow = 0))
model.logloss = data.frame(matrix(ncol = K+2, nrow = 0))
colnames(model.accuracies) = c('model',1:K,"Average")
colnames(model.logloss) = c('model',1:K,"Average")
for (model in models) {
  if (model == 'logistic') {
    formula = as.formula(label~NDAI+CORR+SD+AN)
  } else{
    formula = as.formula(label~.)
  }
  res = CVgeneric(model,data,K,LogLoss,hyperparams,formula,1)
  accuracies = res[['accuracies']]
  average = mean(accuracies)
  model.accuracies[nrow(model.accuracies)+1,]=c(model,accuracies,average)
  model.logloss[nrow(model.logloss)+1,]=c(model,res[['losses']],mean(res[['losses']]))
}
print("METHOD 1:")
for (i in 2:(K+2)) {
  model.accuracies[,i] = as.numeric(model.accuracies[,i])
  model.logloss[,i] = as.numeric(model.logloss[,i])
}
print(model.accuracies, digits = 4)

test.data.2 = data3
train.data.2 = data2
val.data.2 = data1

K=2
data.2 = rbind(train.data.2, val.data.2)
data.2$label = (data.2$label+1)/2
model.accuracies.2 = data.frame(matrix(ncol = K+2, nrow = 0))
model.logloss.2 = data.frame(matrix(ncol = K+2, nrow = 0))
colnames(model.accuracies.2) = c('model',1:K,"Average")
colnames(model.logloss.2) = c('model',1:K,"Average")
for (model in models) {
  if (model == 'logistic') {
    formula = as.formula(label~NDAI+CORR+SD+AN)
  } else{
    formula = as.formula(label~.)
  }
  res = CVgeneric(model,data.2,K,LogLoss,hyperparams,formula,2)
  accuracies = res[['accuracies']]
  average = mean(accuracies)
  model.accuracies.2[nrow(model.accuracies.2)+1,]=c(model,accuracies,average)
  model.logloss.2[nrow(model.logloss.2)+1,]=c(model,res[['losses']],mean(res[['losses']]))
}
cat("\n")
print("METHOD 2:")
for (i in 2:(K+2)) {
  model.accuracies.2[,i] = as.numeric(model.accuracies.2[,i])
  model.logloss.2[,i] = as.numeric(model.logloss.2[,i])
}
print(model.accuracies.2, digits = 4)

trueLabels = (test.data$label+1)/2
data = data[,c(-1,-2,-12)]
model.log = glm(label~NDAI+CORR+SD+AN,family = 'binomial', data = data)
model.log.pred = predict(model.log, test.data, type = 'response')
model.log.acc = mean(round(model.log.pred) == trueLabels)

model.lda = lda(formula, data = data)
model.lda.pred = predict(model.lda, test.data, type = 'response')
model.lda.prediction = as.numeric(model.lda.pred$class)-1
model.lda.acc = mean(round(model.lda.prediction) == trueLabels)

model.qda = qda(formula, data = data)
model.qda.pred = predict(model.qda, test.data, type = 'response')
model.qda.prediction = as.numeric(model.qda.pred$class)-1
model.qda.acc = mean(round(model.qda.prediction) == trueLabels)

data$label = factor(data$label)
model.dtree = rpart(formula, data=data, method = 'class')
model.dtree.pred = predict(model.dtree, test.data, type='prob')[,2]
model.dtree.acc = mean(round(model.dtree.pred) == trueLabels)

cores=detectCores()
cl <- makeCluster(cores[1]-1) 
registerDoParallel(cl)
model.svm = kernelSVM(data=data,formula = formula, hyperparams = hyperparams)
model.svm.pred = predict(model.svm, test.data,type = "raw")
model.svm.acc = mean(model.svm.pred == trueLabels)
stopCluster(cl)

accuracies = c(model.qda.acc,model.lda.acc,model.log.acc,model.dtree.acc,model.svm.acc)
test.accuracies = data.frame("model"=models, "Test Accuracy"=accuracies)
cat("\n")
print("TEST ACCURACIES:")
print(test.accuracies)


@

QDA: Performed with about 90\% accuracy.\\
Assumptions: The features are all normally distributed. 
<<fig.width=10,fig.asp=0.25>>=
library(reshape2)
ggplot(melt(data.labeled[,c(4,5,6,9)]), aes(sample = value)) + stat_qq() + facet_wrap(~variable, scales = 'free_y',nrow = 1)
@
Looking at the qqplots of the features, we can see that all the features except SD have some resemblance to a normal distribution, meaning that this assumption is satisfied. The number of features must also be less than the number of data points, which is satisfied.\\\\

LDA: Performed with about 90\% accuracy, puts emphasis on NDAI and CORR.\\
Assumptions: Same as above (normal distribution of features - satisfied). There is another assumption that the covariance matricies of each class are equivalent. Here we see the norm of the difference between the covariance matricies:
<<>>=
a = cov(data.labeled[data.labeled$label==1,-c(1,2,12)])
b = cov(data.labeled[data.labeled$label==-1,-c(1,2,12)])
norm(a-b)
@
It can be seen that there is a significant difference between the matricies, meaning that QDA might be a better approach over LDA.\\\\

\begin{minipage}[t]{0.5\textwidth}

Decision Tree: We can see here that the tree formed is quite simple. The main feature used is NDAI, and the AN feature seems to have such an insignificant impact that it might be included in the model due to overfitting.\\
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\vspace{-1cm}
<<out.width='50%'>>=
library(rpart.plot)
prp(model.dtree,extra = 106, fallen.leaves = T)
@
\end{minipage}
Assumptions: Since decision trees are nonparametric, there are no assumptions about the underlying distribution. \\

Logistic Regression: AN was arbitrarily chosen as the radiance reading to use in the model.\\
Assumptions: One of the assumptions is that there should be a linear relationship between the logit of the outcome and the features. We observe below that this is the case.
<<>>=
logit = log(model.log.pred/(1-model.log.pred))
print(paste("CORRELATION OF LOGIT OF OUTCOME/FEATURES:",sqrt(summary(lm(logit~NDAI+CORR+AN+SD,data=test.data))$r.squared)))
@
Another assumption is that features should not be highly correlated with each other, which is satisfied since only AN was used from the radiance readings. We also satisfy the assumption that the output is a binary classification label.
\\\\

Kernel SVM: This model performed the best out of the models tested here. The kernel used is the RBF kernel, with arbitrary parameters $C=1,\sigma=1$. The value of C is set by default and the value of sigma was chosen using the sigest function (part of the kernlab package). Because of the computational complexity of this model, it was infeasible to tune the hyperparameters during the scope of this project.\\
Assumptions: No assumptions, as the kernel svm does not make any assumptions about the input data.
\\\\
We can see from the accuracies that using method 1, each of the models provides a similar accuracy of around 90\%. Method 2 has a little lower accuracies, likely due to the fact that there is less data within each of the folds as opposed to the first method. We also see that NDAI, CORR, and AN seem to be features that are important. The AN feature might be seen as important due to possible overfitting of the data.
\item \textbf{Use ROC curves to compare the different methods.} 
Choose a cutoff value and highlight it on the ROC curve. Explain your choice
of the cutoff value. 
<<fig.width=6,results='hide',out.width='60%'>>=
library(pROC)
par(mfrow = c(2,2))
roc(trueLabels, model.log.pred, plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="1-False Positive Percentage", ylab="True Postive Percentage", col="#377eb8", lwd=4, print.thres = T, print.auc.y=70, print.auc = TRUE, main="Logisitic Regression")

roc(trueLabels, model.lda.pred$posterior[,2], plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="1-False Positive Percentage", ylab="True Postive Percentage", col="#4daf4a", lwd=4, print.thres = T, print.auc.y=70, print.auc = TRUE, main="LDA")

roc(trueLabels, model.qda.pred$posterior[,2], plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="1-False Positive Percentage", ylab="True Postive Percentage", col="#cac44d", lwd=4, print.thres = T, print.auc.y=70, print.auc = TRUE, main="QDA")

roc(trueLabels, model.dtree.pred, plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="1-False Positive Percentage", ylab="True Postive Percentage", col="#b8377e", lwd=4, print.thres = T, print.auc.y=70, print.auc = TRUE, main="Decision Tree")
@
All the of ROC curves of the models have a very close AUC. The threshold labeled in each graph is the point where the graph has the highest sum of the sensitivity (true positive rate) and specificity (1-false positive rate). For QDA and LDA, the boundary is defined as $Q_A(x)-Q_B(x)>threshold$ (class $A$).\\
QDA has the highest AUC among the 4 models, making it appear to be the best model. It also matches that the sum of its sensitivity and specificity is the highest among the 4. Looking at the test accracies for method 2, we can also see that QDA has the highest accuracy, which justifies the ROC observations. \\
There is no ROC curve for the kernel SVM method because this method doesn't output any probabilities, just class values.
\item (Bonus) Assess the fit using other relevant metrics.
\\\\ 
We look at the log loss for each of the models (KSVM is not included since the model does not output probabilities.)
<<>>=
model.logloss[1:4,]
@
We can see from the log loss table above that logisitc regression performs has a significantly lower loss as opposed to QDA and Decision trees. The log loss penalizes more for confident misclassificatoins. This means that the logistic regression is not as confident in misclassifications as the other models. This is, however, a trade off with the low CV accuracy obtained using splitting method 2.
\end{enumerate}

\section{Diagnostics (50 pts)}
\begin{enumerate}[label=(\alph*)]
\item Do an in-depth analysis of a good classification model
of your choice by showing some diagnostic plots or information related to
convergence or parameter estimation.\\\\
Based on the ROC plots and accuracies, most of the models had similar performance, with QDA doing marginally better (KSVM is not considered because the only metric we have to compare it is the test accuracy). QDA also has a high accuracy rate with both splitting methods. Therefore, we choose to diagnose the QDA model.
<<fig.width=9, fig.asp=0.4, out.width="78%">>=
num_outputs = 20
qda.accuracies = c()
n = nrow(data)
cutoffs =round(seq(1,n,length.out=num_outputs+1))
coefs = data.frame(matrix(nrow = num_outputs,ncol = 6))
colnames(coefs) = c("mean_0","cov_0","mean_1","cov_1","scale_0","scale_1")
# colnames(coefs)[colnames(coefs)=="(Intercept)"] <- "Intercept"
for (i in 2:(num_outputs+1)) {
  partialData = data[1:cutoffs[i],]
  class0 = partialData[partialData$label == 0,2:ncol(partialData)]
  class1 = partialData[partialData$label == 1,2:ncol(partialData)]
  mean0 = matrix(colMeans(class0), length(colMeans(class0)), 1)
  cov0 = cov(class0)
  mean1 = matrix(colMeans(class1), length(colMeans(class1)), 1)
  cov1 = cov(class1)
  
  model = qda(label~., data = partialData)
  fnorm1 = model$scaling[,,1]
  fnorm2 = model$scaling[,,2]
  coeffs.vals = list(mean0,cov0,mean1,cov1,fnorm1,fnorm2)
  for (j in 1:length(coeffs.vals)) {
    coeffs.vals[j]=norm(coeffs.vals[[j]], type="f")
  }
  
  coefs[i-1,] = unlist(coeffs.vals)
  prediction = predict(model, test.data,type='response')$class
  qda.accuracies = c(qda.accuracies, mean(prediction==trueLabels))
}

graph1 = ggplot(coefs, aes(x=1:num_outputs/num_outputs)) + labs( x="% of training data", y="parameter value") 
graph2 = ggplot(coefs, aes(x=1:num_outputs/num_outputs)) + labs( x="% of training data", y="parameter value")

for (param in names(coefs)[c(1,3,5,6)]) {
  graph1 = graph1 + geom_line(aes_string(y=param, color = shQuote(param)))
}

for (param in names(coefs)[c(2,4)]) {
  graph2 = graph2 + geom_line(aes_string(y=param, color = shQuote(param)))
}

library(ggpubr)
g = ggarrange(graph1,graph2, nrow = 1,ncol=2, legend = "bottom")
annotate_figure(g, top="Parameter convergence of QDA")
@
The parameters of QDA are the mean, the covariance and the scaling matrix of each class. Here we have graphed the frobenius norm of each of these parameters. Looking at Parameter Convergence Graph, each of the parameter converges to the optimal value at about 80\% of the training data. It means that it only takes about 80\% of the training data to stabilize our classifier.\\
We also see that the covariance of each of the classes is quite different, which means that LDA would not be a feasible option.


\begin{minipage}[t]{0.5\textwidth}

<<fig.width=6,fig.asp=0.5>>=
ggplot(data.frame(), aes(x=1:num_outputs/num_outputs,y=qda.accuracies)) + geom_line()+ labs(title = "Test Accuracy of QDA", x="% of training data", y="Test Accuracy")
@


\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\vspace{1cm}
Looking at this plot, we can see that the model might actually be overfitting once it passes 60\% of the data. 
\end{minipage}\\
\item For your best classification model(s), do you notice any patterns in the 
misclassification errors? Again, use quantitative and visual methods of analysis. Do you notice problems in particular regions, or in
specific ranges of feature values?
<<fig.asp=1/3,fig.width=9>>=
p = predict(model.svm, data.labeled)
predicted = (p == factor((data.labeled$label+1)/2))
ggplot(data.labeled, aes(x=x,y=y, color=predicted)) + facet_wrap(~image, nrow = 1)+geom_point() + ggtitle("Accuracy graph for KSVM (x,y)")
ggplot(data.labeled, aes(x=NDAI,y=CORR, color=predicted)) + facet_wrap(~image, nrow = 1)+geom_point() + ggtitle("Accuracy graph for KSVM (NDAI,CORR)")

p = predict(model.qda, data.labeled, type="response")$class
predicted = p == (data.labeled$label+1)/2
ggplot(data.labeled, aes(x=x,y=y, color=predicted)) + facet_wrap(~image, nrow = 1)+geom_point() + ggtitle("Accuracy graph for QDA (x,y)")
ggplot(data.labeled, aes(x=NDAI,y=CORR, color=predicted)) + facet_wrap(~image, nrow = 1)+geom_point() + ggtitle("Accuracy graph for QDA (NDAI,CORR)")
@
From the accuracy graphs based on location, we can see that there are definitely clusters in both KSVM and QDA regression that are entirely misclassified (moreso in QDA). Looking at the specific clusters, it appears that some of the misclassifications look to be more underfitting in certain areas (left side of data2 for QDA) and overfitting in others. For QDA, it seems that lower values of CORR also have more misclassifications. This again might be do to some over/underfitting of the model. \\

From the accuracy graphs based on the NDAI/CORR, we can see that for the KSVM model, most of the misclassifications are are values of NDAI over 1. For logistic regression, we can also see misclassifications in higher NDAI values. The misclassifications for CORR seem to be relatively well spread our over the range of CORR.
\item Based on parts 4(a) and 4(b), can you think of a better classifier?
How well do you think your model will work on future data without expert 
labels?\\\\
Since overfitting is related to having a high variance, we can use a model that uses ensemble methods to reduce the variance. An example of such a model is a random forest. Since a random forest is an ensemble method of a decision tree, it will have the same expectation but a much lower variance (depending on the number of trees used). The results are shown below.
<<fig.asp=9/24,fig.width=9>>=
library(randomForest)

model.randForest = randomForest(formula=formula,data=data)
model.randForest.pred = predict(model.randForest, test.data)
predicted = model.randForest.pred == trueLabels
print(paste("TEST ACCURACY:",mean(predicted)))

p = predict(model.randForest, data.labeled)
predicted = (p == factor((data.labeled$label+1)/2))
ggplot(data.labeled, aes(x=x,y=y, color=predicted)) + facet_wrap(~image, nrow = 1)+geom_point() + ggtitle("Accuracy graph for Random Forest (x,y)")

@
The results suffer less from overfitting than the models specified in parts (a) and (b).\\
This model should work well better with future data without expert labels since it will be less likely to overfit the data and more likely to balance the bias and variance. We can also see that both the test accuracies are relatively high as compared to the other methods. 
\item Do your results in parts 4(a) and 4(b) change as you modify the\\
way of splitting the data?\\
Yes, the answers change based on the splitting method. the answers above were generated using splitting method 1, since it provides access to more training data points. However, even though the data is split up by blocks, there will inherently still be some dependence between the training and test/validation sets. In this regard, method 2 will model a real world situation better, since there is no dependence in pixels in separate images.  \\
Since method 2 gives us less training points, the observed testing points will likely have a lower accuracy rate, as we saw in 3(a). Empirically, it would be better to use method 2 with multiple images, where each image is a separate fold.
\item Write a paragraph for your conclusion.\\
We have tried 5 models for this project: QDA, LDA, Logistic Regression, Decision Tree and Kernel SVM. All of the 5 models did quite well on the CV accuracy using splitting method 1, with Kernel SVM as the highest; QDA stands out and reported the highest CV accuracy when splitting method 2 was used. Since splitting method 2 model a more realistic world situation as there is completely no dependence in pixels in separate images, we believe this might indicate that QDA is a good model on future data. Moreover, the ROC of QDA reports the best AUC and suggests a cutoff of 0; the assumptions of QDA is also met as we discussed above. The model converges at 80\% of the training data. We believe that QDA is the best model of the five. Since QDA might have the problem of overfitting, we also tried to use a more advanced model: Random Forest. We hope to use this model to lower to variance of the prediction. It turned out this model did exceptionally well, reporting a 0.952 test accuracy. It correctly classified many of the mistakes QDA or other models made. We believe that Random Forest will perform outstandingly on the future data without expert labels since it will be less likely to overfit the data and more likely to balance the bias and variance.

\end{enumerate}
\section{Acknowledgement}
Both members had equal contribution to the project (met up to work on the project).\\
Resources: 
\begin{enumerate}
\item https://www.r-bloggers.com/how-to-implement-random-forests-in-r/
\item https://berkeleyml.github.io/static/notes/n25.pdf
\item https://berkeleyml.github.io/static/notes/n18.pdf
\item https://statquest.org/2018/12/17/roc-and-auc-in-r/
\item Yuansi Chen, Raaz Dwivedi, Bin Yu provided help during Office Hours for this project
\end{enumerate}
Procedure: We followed the parts in order:
\begin{enumerate}
\item Took 1 week for data cleaning, processing, and EDA
\item Spent 1 weeks testing different models and went to OH for opinions
\item Spent the remainder of the time diagnosing the models and performing analysis
\item Compiled everything into RNW file at the end using latex
\end{enumerate}

\section{Github}
the project is hosted here: https://github.com/shreysamdani/Cloud-Data-Classification


\end{document}